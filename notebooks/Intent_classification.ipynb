{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srvJSehxYoi9",
        "outputId": "c822e4bd-e5e3-4f0e-81f3-0ef2897ce475"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'triu' from 'scipy.linalg' (/home/bizon/miniconda3/envs/project55/lib/python3.12/site-packages/scipy/linalg/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hstack\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n",
            "File \u001b[0;32m~/miniconda3/envs/project55/lib/python3.12/site-packages/gensim/__init__.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/project55/lib/python3.12/site-packages/gensim/corpora/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/project55/lib/python3.12/site-packages/gensim/corpora/indexedcorpus.py:14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[1;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
            "File \u001b[0;32m~/miniconda3/envs/project55/lib/python3.12/site-packages/gensim/interfaces.py:19\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[1;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
            "File \u001b[0;32m~/miniconda3/envs/project55/lib/python3.12/site-packages/gensim/matutils.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (/home/bizon/miniconda3/envs/project55/lib/python3.12/site-packages/scipy/linalg/__init__.py)"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from enum import Enum\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from nltk import pos_tag, ne_chunk, Tree\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "\n",
        "# nltk.download('maxent_ne_chunker')\n",
        "# nltk.download('words')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# for yong jie\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# df = pd.read_csv('/content/drive/MyDrive/NUS/Y3S2/CS4248/CS4248 Project/raw_data/fulltrain.csv', header=None, names=['Verdict ', 'Text'])\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# for en hao\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./raw_data/fulltrain.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVerdict\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m df_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./raw_data/balancedtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVerdict\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      8\u001b[0m X_train \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# for yong jie\n",
        "# df = pd.read_csv('/content/drive/MyDrive/NUS/Y3S2/CS4248/CS4248 Project/raw_data/fulltrain.csv', header=None, names=['Verdict ', 'Text'])\n",
        "\n",
        "# for en hao\n",
        "df = pd.read_csv('./raw_data/fulltrain.csv', header=None, names=['Verdict', 'Text'])\n",
        "df_test = pd.read_csv('./raw_data/balancedtest.csv', header=None, names=['Verdict', 'Text'])\n",
        "\n",
        "X_train = df['Text']\n",
        "y_train = df['Verdict']\n",
        "X_test = df_test['Text']\n",
        "y_test = df_test['Verdict']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAHCCAYAAADy9P3IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA490lEQVR4nO3deXyNd97/8fex5KSWxJptmhJLEWILo2lrq0xCUyZlprWVVlCaKKKqGVvQ3jHcqE6V269FZ0qpjhpbEbG1TRQhtoqxRKNTiSpyUCKS8/ujd667p0EvEc4Jr+fjcT0e5/p+P+e6PldO8e51XbmOxW632wUAAIBbKuPsBgAAAEoDQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITANyG+Ph4WSwWnT17tsS22aFDB3Xo0KHEtgfg7iA0ATBl0aJFslgsxuLu7i4/Pz+Fh4frnXfe0cWLF4u97eTkZMXHx+vChQsl17DuTsAB8OAiNAG4LZMnT9Y//vEPzZ07V8OGDZMkjRgxQkFBQdq/f3+xtpmcnKxJkyaVeGgCgJJUztkNAChdunTpolatWhnrcXFx2rx5s5555hl169ZNhw8f1kMPPeTEDgHg7uBME4A79tRTT2n8+PH69ttv9dFHHxnj+/fv14svvqg6derI3d1dPj4+GjBggH788UejJj4+XqNHj5YkBQQEGJf/Tp48KUlauHChnnrqKXl5eclqtSowMFBz584tdq8dOnRQkyZNtH//frVv314VKlRQvXr19Omnn0qStm3bpjZt2uihhx5SgwYNtGnTphtu5+zZs3ruuefk4eGh6tWra/jw4bp69apDTXF7v3btmiZMmKDg4GB5enqqYsWKatu2rbZs2eJQd/LkSVksFv33f/+35s+fr7p168pqtap169batWtXke2mp6frueeeU82aNY3jGzt2rEPNf/7zHw0YMEDe3t6yWq1q3LixFixY8Js9Aw8CzjQBKBEvvPCC/vKXv2jjxo0aNGiQJCkxMVEnTpzQSy+9JB8fHx06dEjz58/XoUOHtGPHDlksFnXv3l3//ve/9fHHH2vWrFmqUaOGJKlmzZqSpLlz56px48bq1q2bypUrp9WrV+uVV15RQUGBoqOji9Xr+fPn9cwzz6hnz57685//rLlz56pnz55avHixRowYoSFDhqh3796aPn26/vSnP+nUqVOqXLmywzaee+451a5dWwkJCdqxY4feeecdnT9/Xn//+9+NmuL2brPZ9P7776tXr14aNGiQLl68qA8++EDh4eHauXOnmjdv7lC/ZMkSXbx4US+//LIsFoumTZum7t2768SJEypfvryknwNs27ZtVb58eQ0ePFi1a9fW8ePHtXr1ar311luSpOzsbD322GOyWCyKiYlRzZo19fnnnysqKko2m00jRowo1s8buG/YAcCEhQsX2iXZd+3addMaT09Pe4sWLYz1n376qUjNxx9/bJdk3759uzE2ffp0uyR7RkZGkfobbSM8PNxep06d3+x54sSJdkn2H374wRhr3769XZJ9yZIlxlh6erpdkr1MmTL2HTt2GOMbNmywS7IvXLiwyDa7devmsK9XXnnFLsm+b9++2+69ffv29vbt2xvr169ft+fm5jrUnD9/3u7t7W0fMGCAMZaRkWGXZK9evbr93Llzxvi//vUvuyT76tWrjbF27drZK1eubP/2228dtltQUGC8joqKsvv6+trPnj3rUNOzZ0+7p6fnDY8HeJBweQ5AialUqZLDb9H98t6mq1ev6uzZs3rsscckSXv27DG1zV9uIycnR2fPnlX79u114sQJ5eTkFLvPnj17GusNGjRQlSpV1KhRI7Vp08YYL3x94sSJItv49Zmiwpvi161bd8e9ly1bVm5ubpKkgoICnTt3TtevX1erVq1u+HN7/vnnVbVqVWO9bdu2Dn3/8MMP2r59uwYMGKBHHnnE4b0Wi0WSZLfb9c9//lNdu3aV3W7X2bNnjSU8PFw5OTmmPzPgfsXlOQAl5tKlS/Ly8jLWz507p0mTJmnp0qU6c+aMQ63ZwPPVV19p4sSJSklJ0U8//VRkG56enrfd58MPP2yEhUKenp7y9/cvMib9fDnv1+rXr++wXrduXZUpU8a4F+tOe//www81Y8YMpaenKy8vzxgPCAgoUvvrIFQYoAr7LgxPTZo0uen+fvjhB124cEHz58/X/Pnzb1jz688QeNAQmgCUiO+++045OTmqV6+eMfbcc88pOTlZo0ePVvPmzVWpUiUVFBSoc+fOKigo+M1tHj9+XJ06dVLDhg01c+ZM+fv7y83NTevWrdOsWbNMbeNGypYte1vjdrv9N7f56xB2J71/9NFHevHFFxUZGanRo0fLy8tLZcuWVUJCgo4fP16ifRcq7Kdv377q37//DWuaNm1qenvA/YjQBKBE/OMf/5AkhYeHS/r5LEdSUpImTZqkCRMmGHVHjx4t8t5fB45Cq1evVm5urlatWuVwNuXXv0XmDEePHnU463Ps2DEVFBSodu3aku6s908//VR16tTRihUrHH42EydOLFavderUkSQdPHjwpjU1a9ZU5cqVlZ+fr9DQ0GLtB7jfcU8TgDu2efNmTZkyRQEBAerTp4+k/zv78euzHW+//XaR91esWFGSijzc8kbbyMnJ0cKFC0uq9WKbM2eOw/rf/vY3ST8/x0q6s95v9N6vv/5aKSkpxeq1Zs2aateunRYsWKDMzEyHucJ9lC1bVj169NA///nPG4arH374oVj7Bu4nnGkCcFs+//xzpaen6/r168rOztbmzZuVmJioWrVqadWqVXJ3d5ckeXh4qF27dpo2bZry8vL0u9/9Ths3blRGRkaRbQYHB0uSxo4dq549e6p8+fLq2rWrwsLC5Obmpq5du+rll1/WpUuX9P/+3/+Tl5eXTp8+fU+P+9cyMjLUrVs3de7cWSkpKfroo4/Uu3dvNWvWTJLuqPdnnnlGK1as0LPPPquIiAhlZGRo3rx5CgwM1KVLl4rV7zvvvKMnn3xSLVu21ODBgxUQEKCTJ09q7dq1SktLkyRNnTpVW7ZsUZs2bTRo0CAFBgbq3Llz2rNnjzZt2qRz584Va9/A/YLQBOC2FF5qc3NzU7Vq1RQUFKS3335bL730UpFnGS1ZskTDhg3TnDlzZLfbFRYWps8//1x+fn4Oda1bt9aUKVM0b948rV+/XgUFBcrIyFCDBg306aefaty4cXrttdfk4+OjoUOHqmbNmhowYMA9O+YbWbZsmSZMmKA33nhD5cqVU0xMjKZPn27M30nvL774orKysvQ///M/2rBhgwIDA/XRRx9p+fLl2rp1a7H6bdasmXbs2KHx48dr7ty5unr1qmrVqqXnnnvOqPH29tbOnTs1efJkrVixQu+9956qV6+uxo0b669//Wux9gvcTyz227lTEAAA4AHFPU0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABJ7TVEIKCgr0/fffq3Llyjf9SggAAOBa7Ha7Ll68KD8/P5Upc+tzSYSmEvL9998X+YZ0AABQOpw6dUoPP/zwLWsITSWk8EnIp06dkoeHh5O7AQAAZthsNvn7+xf5RoMbITSVkMJLch4eHoQmAABKGTO31nAjOAAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgQjlnN4CSUfuNtc5uwSlOTo1wdgsAgAcEZ5oAAABMIDQBAACY4NTQtH37dnXt2lV+fn6yWCxauXKlw7zFYrnhMn36dKOmdu3aReanTp3qsJ39+/erbdu2cnd3l7+/v6ZNm1akl+XLl6thw4Zyd3dXUFCQ1q1bd1eOGQAAlE5ODU2XL19Ws2bNNGfOnBvOnz592mFZsGCBLBaLevTo4VA3efJkh7phw4YZczabTWFhYapVq5ZSU1M1ffp0xcfHa/78+UZNcnKyevXqpaioKO3du1eRkZGKjIzUwYMH786BAwCAUsepN4J36dJFXbp0uem8j4+Pw/q//vUvdezYUXXq1HEYr1y5cpHaQosXL9a1a9e0YMECubm5qXHjxkpLS9PMmTM1ePBgSdLs2bPVuXNnjR49WpI0ZcoUJSYm6t1339W8efPu5BABAMB9otTc05Sdna21a9cqKiqqyNzUqVNVvXp1tWjRQtOnT9f169eNuZSUFLVr105ubm7GWHh4uI4cOaLz588bNaGhoQ7bDA8PV0pKyk37yc3Nlc1mc1gAAMD9q9Q8cuDDDz9U5cqV1b17d4fxV199VS1btlS1atWUnJysuLg4nT59WjNnzpQkZWVlKSAgwOE93t7exlzVqlWVlZVljP2yJisr66b9JCQkaNKkSSVxaAAAoBQoNaFpwYIF6tOnj9zd3R3GY2NjjddNmzaVm5ubXn75ZSUkJMhqtd61fuLi4hz2bbPZ5O/vf9f2BwAAnKtUhKYvvvhCR44c0bJly36ztk2bNrp+/bpOnjypBg0ayMfHR9nZ2Q41heuF90HdrOZm90lJktVqvauhDAAAuJZScU/TBx98oODgYDVr1uw3a9PS0lSmTBl5eXlJkkJCQrR9+3bl5eUZNYmJiWrQoIGqVq1q1CQlJTlsJzExUSEhISV4FAAAoDRzami6dOmS0tLSlJaWJknKyMhQWlqaMjMzjRqbzably5dr4MCBRd6fkpKit99+W/v27dOJEye0ePFijRw5Un379jUCUe/eveXm5qaoqCgdOnRIy5Yt0+zZsx0urQ0fPlzr16/XjBkzlJ6ervj4eO3evVsxMTF39wcAAABKDadentu9e7c6duxorBcGmf79+2vRokWSpKVLl8put6tXr15F3m+1WrV06VLFx8crNzdXAQEBGjlypEMg8vT01MaNGxUdHa3g4GDVqFFDEyZMMB43IEmPP/64lixZonHjxukvf/mL6tevr5UrV6pJkyZ36cgBAEBpY7Hb7XZnN3E/sNls8vT0VE5Ojjw8PO75/vnCXgAAbt/t/PtdKu5pAgAAcDZCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACY4NTQtH37dnXt2lV+fn6yWCxauXKlw/yLL74oi8XisHTu3Nmh5ty5c+rTp488PDxUpUoVRUVF6dKlSw41+/fvV9u2beXu7i5/f39NmzatSC/Lly9Xw4YN5e7urqCgIK1bt67EjxcAAJReTg1Nly9fVrNmzTRnzpyb1nTu3FmnT582lo8//thhvk+fPjp06JASExO1Zs0abd++XYMHDzbmbTabwsLCVKtWLaWmpmr69OmKj4/X/PnzjZrk5GT16tVLUVFR2rt3ryIjIxUZGamDBw+W/EEDAIBSyWK32+3ObkKSLBaLPvvsM0VGRhpjL774oi5cuFDkDFShw4cPKzAwULt27VKrVq0kSevXr9fTTz+t7777Tn5+fpo7d67Gjh2rrKwsubm5SZLeeOMNrVy5Uunp6ZKk559/XpcvX9aaNWuMbT/22GNq3ry55s2bZ6p/m80mT09P5eTkyMPDoxg/gTtT+42193yfruDk1AhntwAAKMVu599vl7+naevWrfLy8lKDBg00dOhQ/fjjj8ZcSkqKqlSpYgQmSQoNDVWZMmX09ddfGzXt2rUzApMkhYeH68iRIzp//rxRExoa6rDf8PBwpaSk3LSv3Nxc2Ww2hwUAANy/XDo0de7cWX//+9+VlJSkv/71r9q2bZu6dOmi/Px8SVJWVpa8vLwc3lOuXDlVq1ZNWVlZRo23t7dDTeH6b9UUzt9IQkKCPD09jcXf3//ODhYAALi0cs5u4FZ69uxpvA4KClLTpk1Vt25dbd26VZ06dXJiZ1JcXJxiY2ONdZvNRnACAOA+5tJnmn6tTp06qlGjho4dOyZJ8vHx0ZkzZxxqrl+/rnPnzsnHx8eoyc7OdqgpXP+tmsL5G7FarfLw8HBYAADA/atUhabvvvtOP/74o3x9fSVJISEhunDhglJTU42azZs3q6CgQG3atDFqtm/frry8PKMmMTFRDRo0UNWqVY2apKQkh30lJiYqJCTkbh8SAAAoJZwami5duqS0tDSlpaVJkjIyMpSWlqbMzExdunRJo0eP1o4dO3Ty5EklJSXpj3/8o+rVq6fw8HBJUqNGjdS5c2cNGjRIO3fu1FdffaWYmBj17NlTfn5+kqTevXvLzc1NUVFROnTokJYtW6bZs2c7XFobPny41q9frxkzZig9PV3x8fHavXu3YmJi7vnPBAAAuCanhqbdu3erRYsWatGihSQpNjZWLVq00IQJE1S2bFnt379f3bp106OPPqqoqCgFBwfriy++kNVqNbaxePFiNWzYUJ06ddLTTz+tJ5980uEZTJ6entq4caMyMjIUHBysUaNGacKECQ7Pcnr88ce1ZMkSzZ8/X82aNdOnn36qlStXqkmTJvfuhwEAAFyayzynqbTjOU3OwXOaAAB34r56ThMAAIArIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYEI5ZzcA4PbxBc0AcO9xpgkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEp4am7du3q2vXrvLz85PFYtHKlSuNuby8PI0ZM0ZBQUGqWLGi/Pz81K9fP33//fcO26hdu7YsFovDMnXqVIea/fv3q23btnJ3d5e/v7+mTZtWpJfly5erYcOGcnd3V1BQkNatW3dXjhkAAJROTg1Nly9fVrNmzTRnzpwicz/99JP27Nmj8ePHa8+ePVqxYoWOHDmibt26FamdPHmyTp8+bSzDhg0z5mw2m8LCwlSrVi2lpqZq+vTpio+P1/z5842a5ORk9erVS1FRUdq7d68iIyMVGRmpgwcP3p0DBwAApU45Z+68S5cu6tKlyw3nPD09lZiY6DD27rvv6ve//70yMzP1yCOPGOOVK1eWj4/PDbezePFiXbt2TQsWLJCbm5saN26stLQ0zZw5U4MHD5YkzZ49W507d9bo0aMlSVOmTFFiYqLeffddzZs3ryQOFQAAlHKl6p6mnJwcWSwWValSxWF86tSpql69ulq0aKHp06fr+vXrxlxKSoratWsnNzc3Yyw8PFxHjhzR+fPnjZrQ0FCHbYaHhyslJeXuHQwAAChVnHqm6XZcvXpVY8aMUa9eveTh4WGMv/rqq2rZsqWqVaum5ORkxcXF6fTp05o5c6YkKSsrSwEBAQ7b8vb2NuaqVq2qrKwsY+yXNVlZWTftJzc3V7m5uca6zWa742MEAACuq1SEpry8PD333HOy2+2aO3euw1xsbKzxumnTpnJzc9PLL7+shIQEWa3Wu9ZTQkKCJk2adNe2DwAAXIvLX54rDEzffvutEhMTHc4y3UibNm10/fp1nTx5UpLk4+Oj7Oxsh5rC9cL7oG5Wc7P7pCQpLi5OOTk5xnLq1KnbPTQAAFCKuHRoKgxMR48e1aZNm1S9evXffE9aWprKlCkjLy8vSVJISIi2b9+uvLw8oyYxMVENGjRQ1apVjZqkpCSH7SQmJiokJOSm+7FarfLw8HBYAADA/cupl+cuXbqkY8eOGesZGRlKS0tTtWrV5Ovrqz/96U/as2eP1qxZo/z8fOMeo2rVqsnNzU0pKSn6+uuv1bFjR1WuXFkpKSkaOXKk+vbtawSi3r17a9KkSYqKitKYMWN08OBBzZ49W7NmzTL2O3z4cLVv314zZsxQRESEli5dqt27dzs8lgAAADzYnBqadu/erY4dOxrrhfcn9e/fX/Hx8Vq1apUkqXnz5g7v27Jlizp06CCr1aqlS5cqPj5eubm5CggI0MiRIx3uc/L09NTGjRsVHR2t4OBg1ahRQxMmTDAeNyBJjz/+uJYsWaJx48bpL3/5i+rXr6+VK1eqSZMmd/HoAQBAaWKx2+12ZzdxP7DZbPL09FROTo5TLtXVfmPtPd+nKzg5NcLZLTgFnzcAlIzb+ffbpe9pAgAAcBWEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYUKzQVKdOHf34449Fxi9cuKA6derccVMAAACuplih6eTJk8rPzy8ynpubq//85z+mt7N9+3Z17dpVfn5+slgsWrlypcO83W7XhAkT5Ovrq4ceekihoaE6evSoQ825c+fUp08feXh4qEqVKoqKitKlS5ccavbv36+2bdvK3d1d/v7+mjZtWpFeli9froYNG8rd3V1BQUFat26d6eMAAAD3v3K3U7xq1Srj9YYNG+Tp6Wms5+fnKykpSbVr1za9vcuXL6tZs2YaMGCAunfvXmR+2rRpeuedd/Thhx8qICBA48ePV3h4uL755hu5u7tLkvr06aPTp08rMTFReXl5eumllzR48GAtWbJEkmSz2RQWFqbQ0FDNmzdPBw4c0IABA1SlShUNHjxYkpScnKxevXopISFBzzzzjJYsWaLIyEjt2bNHTZo0uZ0fEQAAuE9Z7Ha73WxxmTI/n5iyWCz69dvKly+v2rVra8aMGXrmmWduvxGLRZ999pkiIyMl/XyWyc/PT6NGjdJrr70mScrJyZG3t7cWLVqknj176vDhwwoMDNSuXbvUqlUrSdL69ev19NNP67vvvpOfn5/mzp2rsWPHKisrS25ubpKkN954QytXrlR6erok6fnnn9fly5e1Zs0ao5/HHntMzZs317x580z1b7PZ5OnpqZycHHl4eNz28d+p2m+svef7dAUnp0Y4uwWn4PMGgJJxO/9+39bluYKCAhUUFOiRRx7RmTNnjPWCggLl5ubqyJEjxQpMN5KRkaGsrCyFhoYaY56enmrTpo1SUlIkSSkpKapSpYoRmCQpNDRUZcqU0ddff23UtGvXzghMkhQeHq4jR47o/PnzRs0v91NYU7gfAACA27o8VygjI6Ok+ygiKytLkuTt7e0w7u3tbcxlZWXJy8vLYb5cuXKqVq2aQ01AQECRbRTOVa1aVVlZWbfcz43k5uYqNzfXWLfZbLdzeAAAoJQpVmiSpKSkJCUlJRlnnH5pwYIFd9yYq0tISNCkSZOc3QaABwCXYwHXUKzfnps0aZLCwsKUlJSks2fP6vz58w5LSfDx8ZEkZWdnO4xnZ2cbcz4+Pjpz5ozD/PXr13Xu3DmHmhtt45f7uFlN4fyNxMXFKScnx1hOnTp1u4cIAABKkWKdaZo3b54WLVqkF154oaT7MQQEBMjHx0dJSUlq3ry5pJ8vgX399dcaOnSoJCkkJEQXLlxQamqqgoODJUmbN29WQUGB2rRpY9SMHTtWeXl5Kl++vCQpMTFRDRo0UNWqVY2apKQkjRgxwth/YmKiQkJCbtqf1WqV1Wot6cMGAAAuqlhnmq5du6bHH3/8jnd+6dIlpaWlKS0tTdLP90qlpaUpMzNTFotFI0aM0JtvvqlVq1bpwIED6tevn/z8/IzfsGvUqJE6d+6sQYMGaefOnfrqq68UExOjnj17ys/PT5LUu3dvubm5KSoqSocOHdKyZcs0e/ZsxcbGGn0MHz5c69ev14wZM5Senq74+Hjt3r1bMTExd3yMAADg/lCs0DRw4EDjOUh3Yvfu3WrRooVatGghSYqNjVWLFi00YcIESdLrr7+uYcOGafDgwWrdurUuXbqk9evXG89okqTFixerYcOG6tSpk55++mk9+eSTmj9/vjHv6empjRs3KiMjQ8HBwRo1apQmTJhgPKNJkh5//HEtWbJE8+fPV7NmzfTpp59q5cqVPKMJAAAYbus5TYWGDx+uv//972ratKmaNm1qXPYqNHPmzBJrsLTgOU3O8aDeKMrn/WDh8wbuntv597tY9zTt37/fuM/o4MGDDnMWi6U4mwQAAHBpxQpNW7ZsKek+AAAAXFqx7mkCAAB40BTrTFPHjh1veRlu8+bNxW4IAADAFRUrNBXez1QoLy9PaWlpOnjwoPr3718SfQEAALiUYoWmWbNm3XA8Pj5ely5duqOGAAAAXFGJ3tPUt2/fB+J75wAAwIOnRENTSkqKw4MnAQAA7hfFujzXvXt3h3W73a7Tp09r9+7dGj9+fIk0BgAA4EqKFZo8PT0d1suUKaMGDRpo8uTJCgsLK5HGAAAAXEmxQtPChQtLug8AAACXVqzQVCg1NVWHDx+WJDVu3Nj44l0AAID7TbFC05kzZ9SzZ09t3bpVVapUkSRduHBBHTt21NKlS1WzZs2S7BEAAMDpivXbc8OGDdPFixd16NAhnTt3TufOndPBgwdls9n06quvlnSPAAAATlesM03r16/Xpk2b1KhRI2MsMDBQc+bM4UZwAABwXyrWmaaCggKVL1++yHj58uVVUFBwx00BAAC4mmKFpqeeekrDhw/X999/b4z95z//0ciRI9WpU6cSaw4AAMBVFCs0vfvuu7LZbKpdu7bq1q2runXrKiAgQDabTX/7299KukcAAACnK9Y9Tf7+/tqzZ482bdqk9PR0SVKjRo0UGhpaos0BAAC4its607R582YFBgbKZrPJYrHoD3/4g4YNG6Zhw4apdevWaty4sb744ou71SsAAIDT3FZoevvttzVo0CB5eHgUmfP09NTLL7+smTNnllhzAAAAruK2QtO+ffvUuXPnm86HhYUpNTX1jpsCAABwNbcVmrKzs2/4qIFC5cqV0w8//HDHTQEAALia2wpNv/vd73Tw4MGbzu/fv1++vr533BQAAICrua3Q9PTTT2v8+PG6evVqkbkrV65o4sSJeuaZZ0qsOQAAAFdxW48cGDdunFasWKFHH31UMTExatCggSQpPT1dc+bMUX5+vsaOHXtXGgUAAHCm2wpN3t7eSk5O1tChQxUXFye73S5JslgsCg8P15w5c+Tt7X1XGgUAAHCm2364Za1atbRu3TqdP39ex44dk91uV/369VW1atW70R8AAIBLKNYTwSWpatWqat26dUn2AgAA4LKK9d1zAAAADxpCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJLh+aateuLYvFUmSJjo6WJHXo0KHI3JAhQxy2kZmZqYiICFWoUEFeXl4aPXq0rl+/7lCzdetWtWzZUlarVfXq1dOiRYvu1SECAIBSoNjfPXev7Nq1S/n5+cb6wYMH9Yc//EF//vOfjbFBgwZp8uTJxnqFChWM1/n5+YqIiJCPj4+Sk5N1+vRp9evXT+XLl9d//dd/SZIyMjIUERGhIUOGaPHixUpKStLAgQPl6+ur8PDwe3CUAADA1bl8aKpZs6bD+tSpU1W3bl21b9/eGKtQoYJ8fHxu+P6NGzfqm2++0aZNm+Tt7a3mzZtrypQpGjNmjOLj4+Xm5qZ58+YpICBAM2bMkCQ1atRIX375pWbNmkVoAgAAkkrB5blfunbtmj766CMNGDBAFovFGF+8eLFq1KihJk2aKC4uTj/99JMxl5KSoqCgIHl7extj4eHhstlsOnTokFETGhrqsK/w8HClpKTctJfc3FzZbDaHBQAA3L9c/kzTL61cuVIXLlzQiy++aIz17t1btWrVkp+fn/bv368xY8boyJEjWrFihSQpKyvLITBJMtazsrJuWWOz2XTlyhU99NBDRXpJSEjQpEmTSvLwAACACytVoemDDz5Qly5d5OfnZ4wNHjzYeB0UFCRfX1916tRJx48fV926de9aL3FxcYqNjTXWbTab/P3979r+AACAc5Wa0PTtt99q06ZNxhmkm2nTpo0k6dixY6pbt658fHy0c+dOh5rs7GxJMu6D8vHxMcZ+WePh4XHDs0ySZLVaZbVai3UsAACg9Ck19zQtXLhQXl5eioiIuGVdWlqaJMnX11eSFBISogMHDujMmTNGTWJiojw8PBQYGGjUJCUlOWwnMTFRISEhJXgEAACgNCsVoamgoEALFy5U//79Va7c/50cO378uKZMmaLU1FSdPHlSq1atUr9+/dSuXTs1bdpUkhQWFqbAwEC98MIL2rdvnzZs2KBx48YpOjraOFM0ZMgQnThxQq+//rrS09P13nvv6ZNPPtHIkSOdcrwAAMD1lIrQtGnTJmVmZmrAgAEO425ubtq0aZPCwsLUsGFDjRo1Sj169NDq1auNmrJly2rNmjUqW7asQkJC1LdvX/Xr18/huU4BAQFau3atEhMT1axZM82YMUPvv/8+jxsAAACGUnFPU1hYmOx2e5Fxf39/bdu27TffX6tWLa1bt+6WNR06dNDevXuL3SMAALi/lYozTQAAAM5GaAIAADCB0AQAAGACoQkAAMCEUnEjOAAAD4rab6x1dgtOcXLqrZ/D6Ao40wQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABggkuHpvj4eFksFoelYcOGxvzVq1cVHR2t6tWrq1KlSurRo4eys7MdtpGZmamIiAhVqFBBXl5eGj16tK5fv+5Qs3XrVrVs2VJWq1X16tXTokWL7sXhAQCAUsSlQ5MkNW7cWKdPnzaWL7/80pgbOXKkVq9ereXLl2vbtm36/vvv1b17d2M+Pz9fERERunbtmpKTk/Xhhx9q0aJFmjBhglGTkZGhiIgIdezYUWlpaRoxYoQGDhyoDRs23NPjBAAArq2csxv4LeXKlZOPj0+R8ZycHH3wwQdasmSJnnrqKUnSwoUL1ahRI+3YsUOPPfaYNm7cqG+++UabNm2St7e3mjdvrilTpmjMmDGKj4+Xm5ub5s2bp4CAAM2YMUOS1KhRI3355ZeaNWuWwsPD7+mxAgAA1+XyZ5qOHj0qPz8/1alTR3369FFmZqYkKTU1VXl5eQoNDTVqGzZsqEceeUQpKSmSpJSUFAUFBcnb29uoCQ8Pl81m06FDh4yaX26jsKZwGzeTm5srm83msAAAgPuXS4emNm3aaNGiRVq/fr3mzp2rjIwMtW3bVhcvXlRWVpbc3NxUpUoVh/d4e3srKytLkpSVleUQmArnC+duVWOz2XTlypWb9paQkCBPT09j8ff3v9PDBQAALsylL8916dLFeN20aVO1adNGtWrV0ieffKKHHnrIiZ1JcXFxio2NNdZtNhvBCQCA+5hLn2n6tSpVqujRRx/VsWPH5OPjo2vXrunChQsONdnZ2cY9UD4+PkV+m65w/bdqPDw8bhnMrFarPDw8HBYAAHD/KlWh6dKlSzp+/Lh8fX0VHBys8uXLKykpyZg/cuSIMjMzFRISIkkKCQnRgQMHdObMGaMmMTFRHh4eCgwMNGp+uY3CmsJtAAAASC4eml577TVt27ZNJ0+eVHJysp599lmVLVtWvXr1kqenp6KiohQbG6stW7YoNTVVL730kkJCQvTYY49JksLCwhQYGKgXXnhB+/bt04YNGzRu3DhFR0fLarVKkoYMGaITJ07o9ddfV3p6ut577z198sknGjlypDMPHQAAuBiXvqfpu+++U69evfTjjz+qZs2aevLJJ7Vjxw7VrFlTkjRr1iyVKVNGPXr0UG5ursLDw/Xee+8Z7y9btqzWrFmjoUOHKiQkRBUrVlT//v01efJkoyYgIEBr167VyJEjNXv2bD388MN6//33edwAAABw4NKhaenSpbecd3d315w5czRnzpyb1tSqVUvr1q275XY6dOigvXv3FqtHAADwYHDpy3MAAACugtAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACa4dGhKSEhQ69atVblyZXl5eSkyMlJHjhxxqOnQoYMsFovDMmTIEIeazMxMRUREqEKFCvLy8tLo0aN1/fp1h5qtW7eqZcuWslqtqlevnhYtWnS3Dw8AAJQiLh2atm3bpujoaO3YsUOJiYnKy8tTWFiYLl++7FA3aNAgnT592limTZtmzOXn5ysiIkLXrl1TcnKyPvzwQy1atEgTJkwwajIyMhQREaGOHTsqLS1NI0aM0MCBA7Vhw4Z7dqwAAMC1lXN2A7eyfv16h/VFixbJy8tLqampateunTFeoUIF+fj43HAbGzdu1DfffKNNmzbJ29tbzZs315QpUzRmzBjFx8fLzc1N8+bNU0BAgGbMmCFJatSokb788kvNmjVL4eHhd+8AAQBAqeHSZ5p+LScnR5JUrVo1h/HFixerRo0aatKkieLi4vTTTz8ZcykpKQoKCpK3t7cxFh4eLpvNpkOHDhk1oaGhDtsMDw9XSkrK3ToUAABQyrj0maZfKigo0IgRI/TEE0+oSZMmxnjv3r1Vq1Yt+fn5af/+/RozZoyOHDmiFStWSJKysrIcApMkYz0rK+uWNTabTVeuXNFDDz1UpJ/c3Fzl5uYa6zabrWQOFAAAuKRSE5qio6N18OBBffnllw7jgwcPNl4HBQXJ19dXnTp10vHjx1W3bt271k9CQoImTZp017YPAABcS6m4PBcTE6M1a9Zoy5Ytevjhh29Z26ZNG0nSsWPHJEk+Pj7Kzs52qClcL7wP6mY1Hh4eNzzLJElxcXHKyckxllOnTt3+gQEAgFLDpUOT3W5XTEyMPvvsM23evFkBAQG/+Z60tDRJkq+vryQpJCREBw4c0JkzZ4yaxMREeXh4KDAw0KhJSkpy2E5iYqJCQkJuuh+r1SoPDw+HBQAA3L9cOjRFR0fro48+0pIlS1S5cmVlZWUpKytLV65ckSQdP35cU6ZMUWpqqk6ePKlVq1apX79+ateunZo2bSpJCgsLU2BgoF544QXt27dPGzZs0Lhx4xQdHS2r1SpJGjJkiE6cOKHXX39d6enpeu+99/TJJ59o5MiRTjt2AADgWlw6NM2dO1c5OTnq0KGDfH19jWXZsmWSJDc3N23atElhYWFq2LChRo0apR49emj16tXGNsqWLas1a9aobNmyCgkJUd++fdWvXz9NnjzZqAkICNDatWuVmJioZs2aacaMGXr//fd53AAAADC49I3gdrv9lvP+/v7atm3bb26nVq1aWrdu3S1rOnTooL17995WfwAA4MHh0meaAAAAXAWhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDT9ypw5c1S7dm25u7urTZs22rlzp7NbAgAALoDQ9AvLli1TbGysJk6cqD179qhZs2YKDw/XmTNnnN0aAABwMkLTL8ycOVODBg3SSy+9pMDAQM2bN08VKlTQggULnN0aAABwMkLT/7p27ZpSU1MVGhpqjJUpU0ahoaFKSUlxYmcAAMAVlHN2A67i7Nmzys/Pl7e3t8O4t7e30tPTi9Tn5uYqNzfXWM/JyZEk2Wy2u9voTRTk/uSU/Tqbs37ezsbn/WDh836w8Hk7Z792u/03awlNxZSQkKBJkyYVGff393dCNw8uz7ed3QHuJT7vBwuf94PF2Z/3xYsX5enpecsaQtP/qlGjhsqWLavs7GyH8ezsbPn4+BSpj4uLU2xsrLFeUFCgc+fOqXr16rJYLHe9X1dhs9nk7++vU6dOycPDw9nt4C7j836w8Hk/WB7Uz9tut+vixYvy8/P7zVpC0/9yc3NTcHCwkpKSFBkZKennIJSUlKSYmJgi9VarVVar1WGsSpUq96BT1+Th4fFA/SF70PF5P1j4vB8sD+Ln/VtnmAoRmn4hNjZW/fv3V6tWrfT73/9eb7/9ti5fvqyXXnrJ2a0BAAAnIzT9wvPPP68ffvhBEyZMUFZWlpo3b67169cXuTkcAAA8eAhNvxITE3PDy3G4MavVqokTJxa5VIn7E5/3g4XP+8HC5/3bLHYzv2MHAADwgOPhlgAAACYQmgAAAEwgNAEAAJhAaAIA3BS3vQL/h9AEALgpq9Wqw4cPO7sNwCXwyAHclsOHD2vHjh0KCQlRw4YNlZ6ertmzZys3N1d9+/bVU0895ewWARTDL78W6pfy8/M1depUVa9eXZI0c+bMe9kW7qIrV64oNTVV1apVU2BgoMPc1atX9cknn6hfv35O6s418cgBmLZ+/Xr98Y9/VKVKlfTTTz/ps88+U79+/dSsWTMVFBRo27Zt2rhxI8HpAXLq1ClNnDhRCxYscHYruENlypRRs2bNinwd1LZt29SqVStVrFhRFotFmzdvdk6DKFH//ve/FRYWpszMTFksFj355JNaunSpfH19Jf38vat+fn7Kz893cqeuhdAE0x5//HE99dRTevPNN7V06VK98sorGjp0qN566y1JP3+JcWpqqjZu3OjkTnGv7Nu3Ty1btuQv1vvA1KlTNX/+fL3//vsO/+NTvnx57du3r8iZCJRuzz77rPLy8rRo0SJduHBBI0aM0DfffKOtW7fqkUceITTdBKEJpnl6eio1NVX16tVTQUGBrFardu7cqRYtWkiSDh48qNDQUGVlZTm5U5SUVatW3XL+xIkTGjVqFH+x3id27dqlvn37qmvXrkpISFD58uUJTfcpb29vbdq0SUFBQZJ+vuH/lVde0bp167RlyxZVrFiR0HQD3NOE22KxWCT9fCrf3d3d4ZuhK1eurJycHGe1hrsgMjJSFovllr9BVfjfBEq/1q1bKzU1VdHR0WrVqpUWL17M53ufunLlisqV+78IYLFYNHfuXMXExKh9+/ZasmSJE7tzXfz2HEyrXbu2jh49aqynpKTokUceMdYzMzON6+G4P/j6+mrFihUqKCi44bJnzx5nt4gSVqlSJX344YeKi4tTaGgoZxruUw0bNtTu3buLjL/77rv64x//qG7dujmhK9dHaIJpQ4cOdfgLtEmTJg7/p/L5559zE/h9Jjg4WKmpqTed/62zUCi9evbsqd27d2vFihWqVauWs9tBCXv22Wf18ccf33Du3XffVa9evfizfQPc0wTgpr744gtdvnxZnTt3vuH85cuXtXv3brVv3/4edwYA9x6hCQAAwAQuzwEAAJhAaAIAADCB0AQAAGACoQkA/pfFYtHKlSud3QYAF0VoAvDAyMrK0rBhw1SnTh1ZrVb5+/ura9euSkpKcnZrAEoBnggO4IFw8uRJPfHEE6pSpYqmT5+uoKAg5eXlacOGDYqOjlZ6erqzWwTg4jjTBOCB8Morr8hisWjnzp3q0aOHHn30UTVu3FixsbHasWPHDd8zZswYPfroo6pQoYLq1Kmj8ePHKy8vz5jft2+fOnbsqMqVK8vDw0PBwcHGU5a//fZbde3aVVWrVlXFihXVuHFjrVu37p4cK4C7gzNNAO57586d0/r16/XWW2+pYsWKRearVKlyw/dVrlxZixYtkp+fnw4cOKBBgwapcuXKev311yVJffr0UYsWLTR37lyVLVtWaWlpKl++vCQpOjpa165d0/bt21WxYkV98803qlSp0l07RgB3H6EJwH3v2LFjstvtatiw4W29b9y4ccbr2rVr67XXXtPSpUuN0JSZmanRo0cb261fv75Rn5mZqR49ehjfIl+nTp07PQwATsblOQD3veJ+8cGyZcv0xBNPyMfHR5UqVdK4ceOUmZlpzMfGxmrgwIEKDQ3V1KlTdfz4cWPu1Vdf1ZtvvqknnnhCEydO1P79++/4OAA4F6EJwH2vfv36slgst3Wzd0pKivr06aOnn35aa9as0d69ezV27Fhdu3bNqImPj9ehQ4cUERGhzZs3KzAwUJ999pkkaeDAgTpx4oReeOEFHThwQK1atdLf/va3Ej82APcO3z0H4IHQpUsXHThwQEeOHClyX9OFCxdUpUoVWSwWffbZZ4qMjNSMGTP03nvvOZw9GjhwoD799FNduHDhhvvo1auXLl++rFWrVhWZi4uL09q1aznjBJRinGkC8ECYM2eO8vPz9fvf/17//Oc/dfToUR0+fFjvvPOOQkJCitTXr19fmZmZWrp0qY4fP6533nnHOIskSVeuXFFMTIy2bt2qb7/9Vl999ZV27dqlRo0aSZJGjBihDRs2KCMjQ3v27NGWLVuMOQClEzeCA3gg1KlTR3v27NFbb72lUaNG6fTp06pZs6aCg4M1d+7cIvXdunXTyJEjFRMTo9zcXEVERGj8+PGKj4+XJJUtW1Y//vij+vXrp+zsbNWoUUPdu3fXpEmTJEn5+fmKjo7Wd999Jw8PD3Xu3FmzZs26l4cMoIRxeQ4AAMAELs8BAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwIT/DxqATxjwE+29AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count the occurrences of each unique value in y_train\n",
        "value_counts = y_train.value_counts()\n",
        "\n",
        "# Plot the data imbalance in a bar chart\n",
        "value_counts.plot(kind='bar')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Data Imbalance')\n",
        "\n",
        "# Display the bar chart\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uij5DUGpHjaA"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "HXXg0Lf315JO",
        "outputId": "4a901c55-1b9a-45bf-ce47-cf5bdaafc4cd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Preprocessor</th>\n",
              "      <th>Before</th>\n",
              "      <th>After</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LOWERCASE</td>\n",
              "      <td>I am going to bed, my school starts at 8:00.</td>\n",
              "      <td>i am going to bed, my school starts at 8:00.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LEMMATIZE</td>\n",
              "      <td>I am going to bed, my school starts at 8:00.</td>\n",
              "      <td>I am going to bed , my school start at 8:00 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>STEM</td>\n",
              "      <td>I am going to bed, my school starts at 8:00.</td>\n",
              "      <td>i am go to bed , my school start at 8:00 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>REMOVE_NUMBERS</td>\n",
              "      <td>I am going to bed, my school starts at 8:00.</td>\n",
              "      <td>I am going to bed, my school starts at :.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>REMOVE_PUNCTUATIONS</td>\n",
              "      <td>I am going to bed, my school starts at 8:00.</td>\n",
              "      <td>I am going to bed my school starts at 800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>REMOVE_STOPWORDS</td>\n",
              "      <td>I am going to bed, my school starts at 8:00.</td>\n",
              "      <td>I going bed , school starts 8:00 .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Preprocessor                                        Before  \\\n",
              "0            LOWERCASE  I am going to bed, my school starts at 8:00.   \n",
              "1            LEMMATIZE  I am going to bed, my school starts at 8:00.   \n",
              "2                 STEM  I am going to bed, my school starts at 8:00.   \n",
              "3       REMOVE_NUMBERS  I am going to bed, my school starts at 8:00.   \n",
              "4  REMOVE_PUNCTUATIONS  I am going to bed, my school starts at 8:00.   \n",
              "5     REMOVE_STOPWORDS  I am going to bed, my school starts at 8:00.   \n",
              "\n",
              "                                           After  \n",
              "0   i am going to bed, my school starts at 8:00.  \n",
              "1  I am going to bed , my school start at 8:00 .  \n",
              "2     i am go to bed , my school start at 8:00 .  \n",
              "3      I am going to bed, my school starts at :.  \n",
              "4      I am going to bed my school starts at 800  \n",
              "5             I going bed , school starts 8:00 .  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Preprocessors\n",
        "def lowercase(text):\n",
        "  return text.lower()\n",
        "\n",
        "def lemmatize(text):\n",
        "  words = word_tokenize(text)\n",
        "  return ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "def stem(text):\n",
        "  words = word_tokenize(text)\n",
        "  return ' '.join([ps.stem(word) for word in words])\n",
        "\n",
        "def remove_numbers(text):\n",
        "  return re.sub(r'\\d+', '', text)\n",
        "\n",
        "def remove_punctuations(text):\n",
        "  return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  words = word_tokenize(text)\n",
        "  return ' '.join([word for word in words if word not in stopwords.words('english')])\n",
        "\n",
        "Preprocessor = {\n",
        "    'LOWERCASE': lowercase,\n",
        "    'LEMMATIZE': lemmatize,\n",
        "    'STEM': stem,\n",
        "    'REMOVE_NUMBERS': remove_numbers,\n",
        "    'REMOVE_PUNCTUATIONS': remove_punctuations,\n",
        "    'REMOVE_STOPWORDS': remove_stopwords\n",
        "}\n",
        "\n",
        "SAMPLE_TEXT = \"I am going to bed, my school starts at 8:00.\"\n",
        "BEFORE_AND_AFTER_PREPROCESS_LIST = [[preprocessor, SAMPLE_TEXT, Preprocessor[preprocessor](SAMPLE_TEXT)] for preprocessor in Preprocessor]\n",
        "pd.DataFrame(BEFORE_AND_AFTER_PREPROCESS_LIST, columns=['Preprocessor', 'Before', 'After'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "98Dp8wtbXxTZ",
        "outputId": "d98d0b2c-5e5b-49cb-8a92-3c41948044a9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>cleaned_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A little less than a decade ago, hockey fans w...</td>\n",
              "      <td>a little less than a decade ago, hockey fans w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The writers of the HBO series The Sopranos too...</td>\n",
              "      <td>the writers of the hbo series the sopranos too...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Despite claims from the TV news outlet to offe...</td>\n",
              "      <td>despite claims from the tv news outlet to offe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>After receiving 'subpar' service and experienc...</td>\n",
              "      <td>after receiving 'subpar' service and experienc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>After watching his beloved Seattle Mariners pr...</td>\n",
              "      <td>after watching his beloved seattle mariners pr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  \\\n",
              "0  A little less than a decade ago, hockey fans w...   \n",
              "1  The writers of the HBO series The Sopranos too...   \n",
              "2  Despite claims from the TV news outlet to offe...   \n",
              "3  After receiving 'subpar' service and experienc...   \n",
              "4  After watching his beloved Seattle Mariners pr...   \n",
              "\n",
              "                                        cleaned_text  \n",
              "0  a little less than a decade ago, hockey fans w...  \n",
              "1  the writers of the hbo series the sopranos too...  \n",
              "2  despite claims from the tv news outlet to offe...  \n",
              "3  after receiving 'subpar' service and experienc...  \n",
              "4  after watching his beloved seattle mariners pr...  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Preprocess text\n",
        "def combine_preprocessors(preprocessors):\n",
        "  def preprocess(text):\n",
        "    cleaned_text = text\n",
        "    for preprocessor in preprocessors:\n",
        "      cleaned_text = preprocessor(cleaned_text)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "  return preprocess\n",
        "\n",
        "df['cleaned_text'] = df['Text'].apply(combine_preprocessors([\n",
        "    Preprocessor['LOWERCASE'],\n",
        "]))\n",
        "\n",
        "df[['Text', 'cleaned_text']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Baseline Model (Count + Tfidf Vectorizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.69      0.79      0.74       750\n",
            "           2       0.63      0.26      0.37       750\n",
            "           3       0.57      0.92      0.70       750\n",
            "           4       0.88      0.71      0.78       750\n",
            "\n",
            "    accuracy                           0.67      3000\n",
            "   macro avg       0.69      0.67      0.65      3000\n",
            "weighted avg       0.69      0.67      0.65      3000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
        "    ('rf', RandomForestClassifier(random_state=42))  # Using a fixed random state for reproducibility\n",
        "])\n",
        "\n",
        "# Train the RandomForest model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = pipeline.predict(X_test)\n",
        "print(classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.64      0.79      0.71       750\n",
            "           2       0.66      0.33      0.44       750\n",
            "           3       0.57      0.87      0.69       750\n",
            "           4       0.89      0.67      0.77       750\n",
            "\n",
            "    accuracy                           0.66      3000\n",
            "   macro avg       0.69      0.66      0.65      3000\n",
            "weighted avg       0.69      0.66      0.65      3000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pipeline = Pipeline([\n",
        "    ('count', CountVectorizer(stop_words='english')),\n",
        "    ('rf', RandomForestClassifier(random_state=42))  # Using a fixed random state for reproducibility\n",
        "])\n",
        "\n",
        "# Train the RandomForest model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = pipeline.predict(X_test)\n",
        "prediction_train = pipeline.predict(X_train)\n",
        "print(classification_report(y_test, predictions))\n",
        "print(classification_report(y_train, prediction_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m y_test_adjusted \u001b[38;5;241m=\u001b[39m y_test \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Train the model on the training data\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_adjusted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Print the best parameters\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters: \u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
            "File \u001b[0;32m~/CS4248-55/.conda/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/CS4248-55/.conda/lib/python3.11/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m~/CS4248-55/.conda/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/CS4248-55/.conda/lib/python3.11/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
            "File \u001b[0;32m~/CS4248-55/.conda/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/CS4248-55/.conda/lib/python3.11/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/CS4248-55/.conda/lib/python3.11/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/CS4248-55/.conda/lib/python3.11/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Define the pipeline with TF-IDF vectorizer and XGBClassifier\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
        "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=200, max_depth=10, random_state=42))  # Adjust XGBoost parameters as needed\n",
        "])\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'xgb__n_estimators': [200, 300, 400],\n",
        "    'xgb__max_depth': [10, 12, 14],\n",
        "    'xgb__learning_rate': [0.3, 0.5, 0.7]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose=1, n_jobs=5)\n",
        "\n",
        "y_train_adjusted = y_train - 1\n",
        "y_test_adjusted = y_test - 1\n",
        "\n",
        "# Train the model on the training data\n",
        "grid_search.fit(X_train, y_train_adjusted)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "predictions = grid_search.predict(X_test)\n",
        "print(classification_report(y_test_adjusted, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.79      0.69      0.74       750\n",
        "           1       0.67      0.42      0.52       750\n",
        "           2       0.58      0.71      0.64       750\n",
        "           3       0.71      0.91      0.80       750\n",
        "\n",
        "    accuracy                           0.68      3000\n",
        "   macro avg       0.69      0.68      0.67      3000\n",
        "weighted avg       0.69      0.68      0.67      3000\n",
        "\n",
        "Best parameters:  {'xgb__learning_rate': 0.3, 'xgb__max_depth': 10, 'xgb__n_estimators': 400}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.67      0.61      0.64       750\n",
            "           2       0.64      0.37      0.46       750\n",
            "           3       0.53      0.83      0.65       750\n",
            "           4       0.80      0.75      0.77       750\n",
            "\n",
            "    accuracy                           0.64      3000\n",
            "   macro avg       0.66      0.64      0.63      3000\n",
            "weighted avg       0.66      0.64      0.63      3000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the text data\n",
        "df['Tokenized_Text'] = df['Text'].apply(word_tokenize)\n",
        "df_test['Tokenized_Text'] = df_test['Text'].apply(word_tokenize)\n",
        "\n",
        "# Train a Word2Vec model (or load a pre-trained model)\n",
        "model_w2v = Word2Vec(sentences=df['Tokenized_Text'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "word_vectors = model_w2v.wv\n",
        "\n",
        "# Function to vectorize a text\n",
        "def vectorize_text(text_tokens, model):\n",
        "    vector = np.zeros(model.vector_size)\n",
        "    num_words = 0\n",
        "    for token in text_tokens:\n",
        "        if token in model.key_to_index:\n",
        "            vector += model[token]\n",
        "            num_words += 1\n",
        "    if num_words > 0:\n",
        "        vector = vector / num_words\n",
        "    return vector\n",
        "\n",
        "# Vectorize the training and testing data\n",
        "X_train = np.array([vectorize_text(tokens, word_vectors) for tokens in df['Tokenized_Text']])\n",
        "X_test = np.array([vectorize_text(tokens, word_vectors) for tokens in df_test['Tokenized_Text']])\n",
        "y_train = df['Verdict']\n",
        "y_test = df_test['Verdict']\n",
        "\n",
        "# Train a RandomForest Classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = clf.predict(X_test)\n",
        "print(classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Preprocessor' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m preprocessor_name, preprocessor_func \u001b[38;5;129;01min\u001b[39;00m \u001b[43mPreprocessor\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Apply preprocessor\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreprocessed_Text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocessor_func)\n\u001b[1;32m      4\u001b[0m     df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreprocessed_Text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocessor_func)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Preprocessor' is not defined"
          ]
        }
      ],
      "source": [
        "for preprocessor_name, preprocessor_func in Preprocessor.items():\n",
        "    # Apply preprocessor\n",
        "    df['Preprocessed_Text'] = df['Text'].apply(preprocessor_func)\n",
        "    df_test['Preprocessed_Text'] = df_test['Text'].apply(preprocessor_func)\n",
        "    \n",
        "    # Prepare data\n",
        "    X_train = df['Preprocessed_Text']\n",
        "    y_train = df['Verdict']\n",
        "    X_test = df_test['Preprocessed_Text']\n",
        "    y_test = df_test['Verdict']\n",
        "    \n",
        "    # Pipeline\n",
        "    pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
        "    ('rf', RandomForestClassifier(random_state=42))  # Using a fixed random state for reproducibility\n",
        "    ])\n",
        "    \n",
        "    # Train\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate\n",
        "    predictions = pipeline.predict(X_test)\n",
        "    \n",
        "    # Results\n",
        "    print(f\"Results for {preprocessor_name}:\")\n",
        "    print(classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results for LOWERCASE:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.69      0.79      0.74       750\n",
        "           2       0.63      0.26      0.37       750\n",
        "           3       0.57      0.92      0.70       750\n",
        "           4       0.88      0.71      0.78       750\n",
        "\n",
        "    accuracy                           0.67      3000\n",
        "   macro avg       0.69      0.67      0.65      3000\n",
        "weighted avg       0.69      0.67      0.65      3000\n",
        "\n",
        "Results for LEMMATIZE:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.70      0.81      0.75       750\n",
        "           2       0.66      0.29      0.40       750\n",
        "           3       0.57      0.92      0.70       750\n",
        "           4       0.87      0.68      0.77       750\n",
        "\n",
        "    accuracy                           0.68      3000\n",
        "   macro avg       0.70      0.68      0.66      3000\n",
        "weighted avg       0.70      0.68      0.66      3000\n",
        "\n",
        "Results for STEM:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.68      0.81      0.73       750\n",
        "           2       0.64      0.30      0.41       750\n",
        "           3       0.56      0.92      0.69       750\n",
        "           4       0.89      0.62      0.73       750\n",
        "\n",
        "    accuracy                           0.66      3000\n",
        "   macro avg       0.69      0.66      0.64      3000\n",
        "weighted avg       0.69      0.66      0.64      3000\n",
        "\n",
        "Results for REMOVE_NUMBERS:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.69      0.77      0.73       750\n",
        "           2       0.65      0.29      0.40       750\n",
        "           3       0.56      0.92      0.70       750\n",
        "           4       0.88      0.70      0.78       750\n",
        "\n",
        "    accuracy                           0.67      3000\n",
        "   macro avg       0.70      0.67      0.65      3000\n",
        "weighted avg       0.70      0.67      0.65      3000\n",
        "\n",
        "Results for REMOVE_PUNCTUATIONS:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.69      0.82      0.75       750\n",
        "           2       0.66      0.28      0.40       750\n",
        "           3       0.56      0.91      0.69       750\n",
        "           4       0.87      0.64      0.74       750\n",
        "\n",
        "    accuracy                           0.67      3000\n",
        "   macro avg       0.69      0.67      0.64      3000\n",
        "weighted avg       0.69      0.67      0.64      3000\n",
        "\n",
        "Results for REMOVE_STOPWORDS:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.70      0.78      0.74       750\n",
        "           2       0.62      0.28      0.39       750\n",
        "           3       0.57      0.92      0.70       750\n",
        "           4       0.89      0.71      0.79       750\n",
        "\n",
        "    accuracy                           0.67      3000\n",
        "   macro avg       0.69      0.67      0.65      3000\n",
        "weighted avg       0.69      0.67      0.65      3000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report for Lowercase + Lemmatize:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.71      0.79      0.75       750\n",
            "           2       0.64      0.30      0.41       750\n",
            "           3       0.56      0.93      0.70       750\n",
            "           4       0.88      0.67      0.76       750\n",
            "\n",
            "    accuracy                           0.67      3000\n",
            "   macro avg       0.70      0.67      0.65      3000\n",
            "weighted avg       0.70      0.67      0.65      3000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Apply combined preprocessing steps\n",
        "combined_preprocessor = combine_preprocessors([lowercase, lemmatize])\n",
        "df['Preprocessed_Text'] = df['Text'].apply(combined_preprocessor)\n",
        "df_test['Preprocessed_Text'] = df_test['Text'].apply(combined_preprocessor)\n",
        "\n",
        "# Prepare data\n",
        "X_train = df['Preprocessed_Text']\n",
        "y_train = df['Verdict']\n",
        "X_test = df_test['Preprocessed_Text']\n",
        "y_test = df_test['Verdict']\n",
        "\n",
        "# Define and train the model\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
        "    ('rf', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = pipeline.predict(X_test)\n",
        "print(\"Classification Report for Lowercase + Lemmatize:\")\n",
        "print(classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K7D7sxtHqRp"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "_36K79CZDc4-",
        "outputId": "092b06ac-2088-4be2-df42-eb6d13c1baea"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature Extractor</th>\n",
              "      <th>Input</th>\n",
              "      <th>Output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>COUNT_WORDS</td>\n",
              "      <td>I am going to bed, my beautiful day starts at ...</td>\n",
              "      <td>17.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>COUNT_UNIQUE_WORDS</td>\n",
              "      <td>I am going to bed, my beautiful day starts at ...</td>\n",
              "      <td>0.941176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>COUNT_ADJECTIVES</td>\n",
              "      <td>I am going to bed, my beautiful day starts at ...</td>\n",
              "      <td>0.117647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>COUNT_NOUNS</td>\n",
              "      <td>I am going to bed, my beautiful day starts at ...</td>\n",
              "      <td>0.235294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>COUNT_VERBS</td>\n",
              "      <td>I am going to bed, my beautiful day starts at ...</td>\n",
              "      <td>0.294118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>COUNT_ADVERBS</td>\n",
              "      <td>I am going to bed, my beautiful day starts at ...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>COUNT_PRONOUNS</td>\n",
              "      <td>I am going to bed, my beautiful day starts at ...</td>\n",
              "      <td>0.176471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>COUNT_MODALS</td>\n",
              "      <td>I am going to bed, my beautiful day starts at ...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>COUNT_CONJUNCTIONS</td>\n",
              "      <td>I am going to bed, my beautiful day starts at ...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>COUNT_PREPOSITIONS</td>\n",
              "      <td>I am going to bed, my beautiful day starts at ...</td>\n",
              "      <td>0.058824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>COUNT_INTERJECTIONS</td>\n",
              "      <td>I am going to bed, my beautiful day starts at ...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>COUNT_DETERMINERS</td>\n",
              "      <td>I am going to bed, my beautiful day starts at ...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>COUNT_CONTRACTIONS</td>\n",
              "      <td>I am going to bed, my beautiful day starts at ...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>COUNT_NAMED_ENTITIES</td>\n",
              "      <td>I am going to bed, my beautiful day starts at ...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>DIALOGUE_PERCENTAGE</td>\n",
              "      <td>I am going to bed, my beautiful day starts at ...</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>TEXT_SUBJECTIVITY</td>\n",
              "      <td>I am going to bed, my beautiful day starts at ...</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>TEXT_POLARITY</td>\n",
              "      <td>I am going to bed, my beautiful day starts at ...</td>\n",
              "      <td>0.850000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Feature Extractor                                              Input  \\\n",
              "0            COUNT_WORDS  I am going to bed, my beautiful day starts at ...   \n",
              "1     COUNT_UNIQUE_WORDS  I am going to bed, my beautiful day starts at ...   \n",
              "2       COUNT_ADJECTIVES  I am going to bed, my beautiful day starts at ...   \n",
              "3            COUNT_NOUNS  I am going to bed, my beautiful day starts at ...   \n",
              "4            COUNT_VERBS  I am going to bed, my beautiful day starts at ...   \n",
              "5          COUNT_ADVERBS  I am going to bed, my beautiful day starts at ...   \n",
              "6         COUNT_PRONOUNS  I am going to bed, my beautiful day starts at ...   \n",
              "7           COUNT_MODALS  I am going to bed, my beautiful day starts at ...   \n",
              "8     COUNT_CONJUNCTIONS  I am going to bed, my beautiful day starts at ...   \n",
              "9     COUNT_PREPOSITIONS  I am going to bed, my beautiful day starts at ...   \n",
              "10   COUNT_INTERJECTIONS  I am going to bed, my beautiful day starts at ...   \n",
              "11     COUNT_DETERMINERS  I am going to bed, my beautiful day starts at ...   \n",
              "12    COUNT_CONTRACTIONS  I am going to bed, my beautiful day starts at ...   \n",
              "13  COUNT_NAMED_ENTITIES  I am going to bed, my beautiful day starts at ...   \n",
              "14   DIALOGUE_PERCENTAGE  I am going to bed, my beautiful day starts at ...   \n",
              "15     TEXT_SUBJECTIVITY  I am going to bed, my beautiful day starts at ...   \n",
              "16         TEXT_POLARITY  I am going to bed, my beautiful day starts at ...   \n",
              "\n",
              "       Output  \n",
              "0   17.000000  \n",
              "1    0.941176  \n",
              "2    0.117647  \n",
              "3    0.235294  \n",
              "4    0.294118  \n",
              "5    0.000000  \n",
              "6    0.176471  \n",
              "7    0.000000  \n",
              "8    0.000000  \n",
              "9    0.058824  \n",
              "10   0.000000  \n",
              "11   0.000000  \n",
              "12   0.000000  \n",
              "13   0.000000  \n",
              "14   0.285714  \n",
              "15   1.000000  \n",
              "16   0.850000  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def count_words(text):\n",
        "  words = text.split()\n",
        "  return len(text.split())\n",
        "\n",
        "def count_unique_words(text):\n",
        "  words = text.split()\n",
        "  unique_words = set(words)\n",
        "  return len(unique_words) / count_words(text)\n",
        "\n",
        "def count_adjectives(text):\n",
        "  words = word_tokenize(text)\n",
        "  tagged = nltk.pos_tag(words)\n",
        "  return len([word for word, pos in tagged if pos in ['JJ', 'JJR', 'JJS']]) / count_words(text)\n",
        "    \n",
        "def count_nouns(text):\n",
        "    tags = pos_tag(word_tokenize(text))\n",
        "    return sum(1 for word, tag in tags if tag in ['NN', 'NNS', 'NNP', 'NNPS']) / count_words(text)\n",
        "\n",
        "def count_verbs(text):\n",
        "    tags = pos_tag(word_tokenize(text))\n",
        "    return sum(1 for word, tag in tags if tag.startswith('VB')) / count_words(text)\n",
        "\n",
        "def count_adverbs(text):\n",
        "    tags = pos_tag(word_tokenize(text))\n",
        "    return sum(1 for word, tag in tags if tag in ['RB', 'RBR', 'RBS']) / count_words(text)\n",
        "\n",
        "def count_pronouns(text):\n",
        "    tags = pos_tag(word_tokenize(text))\n",
        "    return sum(1 for word, tag in tags if tag in ['PRP', 'PRP$', 'WP', 'WP$']) / count_words(text)\n",
        "\n",
        "def count_modals(text):\n",
        "    tags = pos_tag(word_tokenize(text))\n",
        "    return sum(1 for word, tag in tags if tag == 'MD') / count_words(text)\n",
        "\n",
        "def count_conjunctions(text):\n",
        "    tags = pos_tag(word_tokenize(text))\n",
        "    return sum(1 for word, tag in tags if tag == 'CC') / count_words(text)\n",
        "\n",
        "def count_prepositions(text):\n",
        "    tags = pos_tag(word_tokenize(text))\n",
        "    return sum(1 for word, tag in tags if tag == 'IN') / count_words(text)\n",
        "\n",
        "def count_interjections(text):\n",
        "    tags = pos_tag(word_tokenize(text))\n",
        "    return sum(1 for word, tag in tags if tag == 'UH') / count_words(text)\n",
        "\n",
        "def count_determiners(text):\n",
        "    tags = pos_tag(word_tokenize(text))\n",
        "    return sum(1 for word, tag in tags if tag in ['DT', 'PDT']) / count_words(text)\n",
        "\n",
        "def count_contractions(text):\n",
        "    count = 0\n",
        "    count += re.subn(r\"won't\", '', text)[1]\n",
        "    count += re.subn(r\"can't\", '', text)[1]\n",
        "    count += re.subn(r\"n't\", '', text)[1]\n",
        "    count += re.subn(r\"'re\", '', text)[1]\n",
        "    count += re.subn(r\"'s\", '', text)[1]\n",
        "    count += re.subn(r\"'d\", '', text)[1]\n",
        "    count += re.subn(r\"'ll\", '', text)[1]\n",
        "    count += re.subn(r\"'t\", '', text)[1]\n",
        "    count += re.subn(r\"'ve\", '', text)[1]\n",
        "    count += re.subn(r\"'m\", '', text)[1]\n",
        "    return count / count_words(text)\n",
        "\n",
        "def count_named_entities(text):\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    # Tokenize sentences into words and tag part of speech\n",
        "    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "    pos_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
        "    # Chunk sentences to identify named entities\n",
        "    chunked_sentences = [ne_chunk(sentence) for sentence in pos_sentences]\n",
        "    # Count the named entities\n",
        "    named_entities_count = sum(1 for sentence in chunked_sentences for chunk in sentence if isinstance(chunk, Tree))\n",
        "    return named_entities_count / count_words(text)\n",
        "\n",
        "def dialogue_percentage(text):\n",
        "    dialogues = re.findall(r'[\"\\']([^\"\\']*)[\"\\']', text)\n",
        "    dialogue_chars = sum(len(dialogue) for dialogue in dialogues)\n",
        "    total_chars = len(text)\n",
        "    \n",
        "    if total_chars > 0:\n",
        "        return (dialogue_chars / total_chars) \n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def text_subjectivity(text):\n",
        "    return TextBlob(text).subjectivity\n",
        "\n",
        "def text_polarity(text):\n",
        "    return TextBlob(text).polarity\n",
        "\n",
        "FeatureExtractor = {\n",
        "    'COUNT_WORDS': count_words,\n",
        "    'COUNT_UNIQUE_WORDS': count_unique_words,\n",
        "    'COUNT_ADJECTIVES': count_adjectives,\n",
        "    'COUNT_NOUNS': count_nouns,\n",
        "    'COUNT_VERBS': count_verbs,\n",
        "    'COUNT_ADVERBS': count_adverbs,\n",
        "    'COUNT_PRONOUNS': count_pronouns,\n",
        "    'COUNT_MODALS': count_modals,\n",
        "    'COUNT_CONJUNCTIONS': count_conjunctions,\n",
        "    'COUNT_PREPOSITIONS': count_prepositions,\n",
        "    'COUNT_INTERJECTIONS': count_interjections,\n",
        "    'COUNT_DETERMINERS': count_determiners,\n",
        "    'COUNT_CONTRACTIONS': count_contractions,\n",
        "    'COUNT_NAMED_ENTITIES': count_named_entities,\n",
        "    'DIALOGUE_PERCENTAGE': dialogue_percentage,\n",
        "    'TEXT_SUBJECTIVITY': text_subjectivity,\n",
        "    'TEXT_POLARITY': text_polarity\n",
        "}\n",
        "\n",
        "SAMPLE_TEXT = \"I am going to bed, my beautiful day starts at 8:00. 'hi my name is yong jie' \"\n",
        "FEATURE_EXTRACTION_OUTPUT_LIST = [[feature_extractor, SAMPLE_TEXT, FeatureExtractor[feature_extractor](SAMPLE_TEXT)] for feature_extractor in FeatureExtractor]\n",
        "pd.DataFrame(FEATURE_EXTRACTION_OUTPUT_LIST, columns=['Feature Extractor', 'Input', 'Output'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "u-JpqvsrHz5Q",
        "outputId": "a5f18ef1-f7fe-4f5e-e2b9-17c9564e72ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word_count\n",
            "unique_word_count\n",
            "adjective_count\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature_col \u001b[38;5;129;01min\u001b[39;00m feature_column_to_feature_extractor:\n\u001b[1;32m     25\u001b[0m   \u001b[38;5;28mprint\u001b[39m(feature_col)\n\u001b[0;32m---> 26\u001b[0m   df[feature_col] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mText\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_column_to_feature_extractor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(PRIMARY_FEATURE_COLUMNS\u001b[38;5;241m.\u001b[39mvalues())]\u001b[38;5;241m.\u001b[39mhead()\n",
            "File \u001b[0;32m~/CS4248-55/.conda/lib/python3.11/site-packages/pandas/core/series.py:4915\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4782\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4791\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4906\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4909\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4913\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/CS4248-55/.conda/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/CS4248-55/.conda/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
            "File \u001b[0;32m~/CS4248-55/.conda/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/CS4248-55/.conda/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
            "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "Cell \u001b[0;32mIn[29], line 12\u001b[0m, in \u001b[0;36mcount_adjectives\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_adjectives\u001b[39m(text):\n\u001b[1;32m     11\u001b[0m   words \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[0;32m---> 12\u001b[0m   tagged \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m([word \u001b[38;5;28;01mfor\u001b[39;00m word, pos \u001b[38;5;129;01min\u001b[39;00m tagged \u001b[38;5;28;01mif\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJJ\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJJR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJJS\u001b[39m\u001b[38;5;124m'\u001b[39m]]) \u001b[38;5;241m/\u001b[39m count_words(text)\n",
            "File \u001b[0;32m~/CS4248-55/.conda/lib/python3.11/site-packages/nltk/tag/__init__.py:166\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03mtag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/CS4248-55/.conda/lib/python3.11/site-packages/nltk/tag/__init__.py:123\u001b[0m, in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens: expected a list of strings, got a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     tagged_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tagset:  \u001b[38;5;66;03m# Maps to the specified tagset.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m~/CS4248-55/.conda/lib/python3.11/site-packages/nltk/tag/perceptron.py:187\u001b[0m, in \u001b[0;36mPerceptronTagger.tag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tag:\n\u001b[1;32m    186\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_features(i, word, context, prev, prev2)\n\u001b[0;32m--> 187\u001b[0m     tag, conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_conf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m output\u001b[38;5;241m.\u001b[39mappend((word, tag, conf) \u001b[38;5;28;01mif\u001b[39;00m return_conf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (word, tag))\n\u001b[1;32m    190\u001b[0m prev2 \u001b[38;5;241m=\u001b[39m prev\n",
            "File \u001b[0;32m~/CS4248-55/.conda/lib/python3.11/site-packages/nltk/tag/perceptron.py:66\u001b[0m, in \u001b[0;36mAveragedPerceptron.predict\u001b[0;34m(self, features, return_conf)\u001b[0m\n\u001b[1;32m     64\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[feat]\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m label, weight \u001b[38;5;129;01min\u001b[39;00m weights\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 66\u001b[0m         scores[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m*\u001b[39m weight\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Do a secondary alphabetic sort, for stability\u001b[39;00m\n\u001b[1;32m     69\u001b[0m best_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m label: (scores[label], label))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Extract primary features\n",
        "feature_column_to_feature_extractor = {\n",
        "    'word_count': FeatureExtractor['COUNT_WORDS'],\n",
        "    'unique_word_count': FeatureExtractor['COUNT_UNIQUE_WORDS'],\n",
        "    'adjective_count': FeatureExtractor['COUNT_ADJECTIVES'],\n",
        "    'noun_count': FeatureExtractor['COUNT_NOUNS'],\n",
        "    'verb_count': FeatureExtractor['COUNT_VERBS'],\n",
        "    'adverb_count': FeatureExtractor['COUNT_ADVERBS'],\n",
        "    'pronoun_count': FeatureExtractor['COUNT_PRONOUNS'],\n",
        "    'modal_count': FeatureExtractor['COUNT_MODALS'],\n",
        "    'conjunction_count': FeatureExtractor['COUNT_CONJUNCTIONS'],\n",
        "    'preposition_count': FeatureExtractor['COUNT_PREPOSITIONS'],\n",
        "    'interjection_count': FeatureExtractor['COUNT_INTERJECTIONS'],\n",
        "    'determiner_count': FeatureExtractor['COUNT_DETERMINERS'],\n",
        "    'contraction_count': FeatureExtractor['COUNT_CONTRACTIONS'],\n",
        "    'named_entity_count': FeatureExtractor['COUNT_NAMED_ENTITIES'],\n",
        "    'dialogue_percentage': FeatureExtractor['DIALOGUE_PERCENTAGE'],\n",
        "    'text_subjectivity': FeatureExtractor['TEXT_SUBJECTIVITY'],\n",
        "    'text_polarity': FeatureExtractor['TEXT_POLARITY']\n",
        "}\n",
        "\n",
        "PRIMARY_FEATURE_COLUMNS = {key.upper(): key for key in feature_column_to_feature_extractor.keys()}\n",
        "\n",
        "for feature_col in feature_column_to_feature_extractor:\n",
        "  print(feature_col)\n",
        "  df[feature_col] = df['Text'].apply(feature_column_to_feature_extractor[feature_col])\n",
        "\n",
        "df[['Text'] + list(PRIMARY_FEATURE_COLUMNS.values())].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jE5vdrtySVeS",
        "outputId": "e18a0fdc-694d-42dd-8d99-fb70e5da7123"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df[['Text'] + list(SECONDARY_FEATURE_COLUMNS\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The writers of the HBO series The Sopranos took another daring storytelling step by killing off 10 million fans during the seventh season's premiere episode Sunday night. 'This was definitely a bold choice, one that producers of the show would have never thought of making five years ago,' said New York Times television critic Virginia Heffernan, who noted that the move was hinted at in a season-five episode in which Tony dreamt he was riding a horse through his house. 'But now that I look back, this was strongly foreshadowed throughout all of last season.' Industry insiders predicted that the show's producers would try to bring at least some fans back for the series finale, which may come as early as May. \",\n          \"After watching his beloved Seattle Mariners prevail against the San Diego Padres, third-grader Timmy Hastert was moved to ask his father, 46-year-old insurance salesman Christopher Hastert, why interleague play is 'good.' 'Well, it lets people see the teams they normally don't get to see all that often, I think is the point, there, buddy,' Hastert said after beginning three different sentences in seven minutes. 'After all, without interleague play, we wouldn't get to see players like like Brian Giles and Scott [Linebrink], would we? Although I think we play the Yankees and Red Sox less often as a result. Right? Yeah, I'm pretty sure that's how they do it.' In a moving but ultimately doomed effort to give his impressionable boy the right messages, Christopher also attempted to answer Timmy's questions regarding why only the American League has a DH, why and how the All-Star game now 'counts,' what performance-enhancing drugs are, and how baseball officials could have sat idly by when they knew there was a major steroid problem in their sport. \",\n          \"Despite claims from the TV news outlet to offer 'nonstop news' and 'coverage you can count on,' an selfnews investigation has uncovered hundreds of instances in which KAMR Channel 4 10 OClock Eyewitness News team relied almost exclusively on news reports, weather forecasts, and even special-interest features already generated by the stations 6 OClock Eyewitness News team. The investigation found that 10 OClock News Team is in fact not the 'team you can trust.' In an examination of 98 consecutive prime-time and late-night broadcasts, including dozens more nationwide, the Amarillo-based stationthe regions self-styled 'News Leader'repeatedly ran pieces for its Health Beat, Pet Patrol, and Bargain Busters segments in both evening news slots, and regularly relayed the same weather updates and traffic reports up to 15 times a day. KAMR even routinely rehashed 6 p.m. footage for seemingly urgent 'breaking news' reports, most recently the Plum Creek Nursing Home power outage and the Bonham Middle School roof collapse. In an April incident involving the 10 p.m. recap of a local Cancer Fun Run, anchor Andy Justus read almost the exact same copy introducing the piece as he had just four hours earlier, while reporter Shalandys Anderson altered only one word between broadcasts, changing 'heartwarming' to 'inspiring.' 'If theyre on our side, as they claim, what, then, is a purportedly professional news team doing in the four hours between broadcasts?' Amarillo resident and frequent local-news viewer Mark Jette said. During another 10 p.m. broadcast, 'live continuing coverage' from reporter Matt Orlando of a two-alarm Elwood Park house fire consisted almost wholly of previously aired footage of the firefighters in action. The lack of updated footage disappointed viewers such as Hereford, TXs Kelly Byer, whose mild curiosity about the blaze, first piqued at the 6 p.m. newscast, went ungratified. <h3>'We'll continue to watch this important breaking news story.'</h3><p>Elizabeth Dinh, after a report on a broken gas main, which had already run in two previous newscasts</p> 'Its true that the image of that scorched little doll was powerful and may have bore repeating, but where was the follow-up footage of the devastated family at a Red Cross shelter?' Byer said. 'Or some fresh b-roll of the charred ruins of the house? The public deserves better.' The investigation also found the 10 p.m. KAMR broadcast consistently re-aired closing stock numbers and high-school baseball highlights and sports bloopers, its producers and anchors apparently unaware or indifferent to the fact that the information was hours old and already common knowledge among viewers. 'They say that the 6 oclock news team is the areas most watched news team,' Jette said. 'Especially by the 10 oclock news team.' Just last Tuesday, investigative reporter Meaghan Colliers 'Problem Solvers' segment on squalid conditions at a local dog kennel aired again at 10 p.m. without even a cursory update on the broken-legged puppy featured in the report. 'Their Scorching Summer coverage was even worse,' Jette continued. 'How many times do you have to repeat the same cool tips before all of Amarillo is crystal clear on exactly how to beat the heat?' While KAMR was a particularly flagrant offender, it is by no means alone. In a segment about the San Diego Zoos baby pandas, KFMB- TV-8s News At 11 not only offered footage identical to the previous telecast, but practically indistinguishable coos of affection from the co-anchor. At some stations, the problem goes far beyond one-time reuse. An 11 p.m. segment on heart-smart dinner alternatives on New Haven, Connecticuts WTNH Channel 8 was not only previously seen on the 6 p.m. news, but also on Live At 5, The 4 Report, and The News At Noon With Sonia Baghdady. Another piece on the citys aging school buses was rotated into the following days Good Morning New Haven! as well. In extreme examples, such as in Louisville, KYs WLKY-TV prime-time and late-night newscasts, the only distinguishable characteristic is the lead anchors concluding suggestion to 'stay tuned for [David Lettermans] The Late Show.' Despite the mounting controversy over the KAMR Channel 4 team, the investigation was unable to conclusively prove that the hopeful wishes to see viewers the next day, and the camaraderie and laughter shared between co-anchors Kyla Cullinane and Elizabeth Dinh, were anything less than genuine. \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"unique_word_count_vs_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.012185240960552424,\n        \"min\": 0.9702127659574468,\n        \"max\": 1.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.9918032786885246,\n          0.9900709219858156,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-a85b5892-c90a-4600-bc4d-2d838a8eecd8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>unique_word_count_vs_word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A little less than a decade ago, hockey fans w...</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The writers of the HBO series The Sopranos too...</td>\n",
              "      <td>0.991803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Despite claims from the TV news outlet to offe...</td>\n",
              "      <td>0.970213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>After receiving 'subpar' service and experienc...</td>\n",
              "      <td>0.990071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>After watching his beloved Seattle Mariners pr...</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a85b5892-c90a-4600-bc4d-2d838a8eecd8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a85b5892-c90a-4600-bc4d-2d838a8eecd8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a85b5892-c90a-4600-bc4d-2d838a8eecd8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-84765774-583a-45d6-b7d5-e69d13ee66e8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-84765774-583a-45d6-b7d5-e69d13ee66e8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-84765774-583a-45d6-b7d5-e69d13ee66e8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                Text  \\\n",
              "0  A little less than a decade ago, hockey fans w...   \n",
              "1  The writers of the HBO series The Sopranos too...   \n",
              "2  Despite claims from the TV news outlet to offe...   \n",
              "3  After receiving 'subpar' service and experienc...   \n",
              "4  After watching his beloved Seattle Mariners pr...   \n",
              "\n",
              "   unique_word_count_vs_word_count  \n",
              "0                         1.000000  \n",
              "1                         0.991803  \n",
              "2                         0.970213  \n",
              "3                         0.990071  \n",
              "4                         1.000000  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate secondary features\n",
        "df['unique_word_count_vs_word_count'] = df[PRIMARY_FEATURE_COLUMNS['UNIQUE_WORD_COUNT']] / df[PRIMARY_FEATURE_COLUMNS['WORD_COUNT']]\n",
        "\n",
        "SECONDARY_FEATURE_COLUMNS = {\n",
        "    'UNIQUE_WORD_COUNT_VS_WORD_COUNT': 'unique_word_count_vs_word_count'\n",
        "}\n",
        "\n",
        "df[['Text'] + list(SECONDARY_FEATURE_COLUMNS.values())].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'lemmatize' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mapply(\u001b[43mlemmatize\u001b[49m)\n\u001b[1;32m      2\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mapply(lemmatize)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lemmatize' is not defined"
          ]
        }
      ],
      "source": [
        "X_train = X_train.apply(lemmatize)\n",
        "X_test = X_test.apply(lemmatize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Function to apply a feature extractor and return the feature as a DataFrame\n",
        "def extract_feature_series(X, feature_func):\n",
        "    return X.apply(feature_func).to_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for DIALOGUE_PERCENTAGE:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.65      0.77      0.71       750\n",
            "           2       0.62      0.30      0.41       750\n",
            "           3       0.55      0.86      0.67       750\n",
            "           4       0.88      0.69      0.77       750\n",
            "\n",
            "    accuracy                           0.66      3000\n",
            "   macro avg       0.68      0.66      0.64      3000\n",
            "weighted avg       0.68      0.66      0.64      3000\n",
            "\n",
            "Results for TEXT_SUBJECTIVITY:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.69      0.83      0.75       750\n",
            "           2       0.67      0.31      0.42       750\n",
            "           3       0.57      0.92      0.71       750\n",
            "           4       0.88      0.67      0.76       750\n",
            "\n",
            "    accuracy                           0.68      3000\n",
            "   macro avg       0.71      0.68      0.66      3000\n",
            "weighted avg       0.71      0.68      0.66      3000\n",
            "\n",
            "Results for TEXT_POLARITY:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.68      0.81      0.74       750\n",
            "           2       0.63      0.28      0.39       750\n",
            "           3       0.55      0.91      0.69       750\n",
            "           4       0.89      0.65      0.75       750\n",
            "\n",
            "    accuracy                           0.66      3000\n",
            "   macro avg       0.69      0.66      0.64      3000\n",
            "weighted avg       0.69      0.66      0.64      3000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "\n",
        "for feature_name, feature_func in FeatureExtractor.items():\n",
        "    # Apply the feature extractor function to training and test sets\n",
        "    train_feature = extract_feature_series(X_train, feature_func)\n",
        "    test_feature = extract_feature_series(X_test, feature_func)\n",
        "    \n",
        "    # Combine the TF-IDF features with the extracted feature\n",
        "    X_train_combined = np.hstack((X_train_tfidf.toarray(), train_feature))\n",
        "    X_test_combined = np.hstack((X_test_tfidf.toarray(), test_feature))\n",
        "    \n",
        "    # Train the RandomForest model\n",
        "    rf_model = RandomForestClassifier(random_state=42)\n",
        "    rf_model.fit(X_train_combined, y_train)\n",
        "    \n",
        "    # Make predictions and evaluate the model\n",
        "    predictions = rf_model.predict(X_test_combined)\n",
        "    report = classification_report(y_test, predictions, output_dict=True)\n",
        "    \n",
        "    # Store the report for later analysis\n",
        "    results[feature_name] = report['accuracy']  # You can store other metrics of interest\n",
        "    \n",
        "    # Print out the classification report for this feature\n",
        "    print(f\"Results for {feature_name}:\")\n",
        "    print(classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results for COUNT_WORDS:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.69      0.81      0.74       750\n",
        "           2       0.64      0.29      0.40       750\n",
        "           3       0.57      0.91      0.70       750\n",
        "           4       0.89      0.67      0.77       750\n",
        "\n",
        "    accuracy                           0.67      3000\n",
        "   macro avg       0.70      0.67      0.65      3000\n",
        "weighted avg       0.70      0.67      0.65      3000\n",
        "\n",
        "Results for COUNT_UNIQUE_WORDS:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.69      0.82      0.75       750\n",
        "           2       0.67      0.30      0.41       750\n",
        "           3       0.57      0.90      0.70       750\n",
        "           4       0.86      0.67      0.76       750\n",
        "\n",
        "    accuracy                           0.67      3000\n",
        "   macro avg       0.70      0.67      0.65      3000\n",
        "weighted avg       0.70      0.67      0.65      3000\n",
        "\n",
        "Results for COUNT_ADJECTIVES:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.68      0.83      0.75       750\n",
        "           2       0.67      0.30      0.41       750\n",
        "           3       0.58      0.92      0.71       750\n",
        "           4       0.90      0.66      0.76       750\n",
        "\n",
        "    accuracy                           0.68      3000\n",
        "   macro avg       0.71      0.68      0.66      3000\n",
        "weighted avg       0.71      0.68      0.66      3000\n",
        "\n",
        "Results for COUNT_NOUNS:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.69      0.83      0.75       750\n",
        "           2       0.67      0.31      0.42       750\n",
        "           3       0.56      0.90      0.69       750\n",
        "           4       0.87      0.66      0.75       750\n",
        "\n",
        "    accuracy                           0.67      3000\n",
        "   macro avg       0.70      0.67      0.65      3000\n",
        "weighted avg       0.70      0.67      0.65      3000\n",
        "\n",
        "Results for COUNT_VERBS:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.69      0.81      0.75       750\n",
        "           2       0.66      0.30      0.41       750\n",
        "           3       0.56      0.91      0.69       750\n",
        "           4       0.88      0.66      0.75       750\n",
        "\n",
        "    accuracy                           0.67      3000\n",
        "   macro avg       0.70      0.67      0.65      3000\n",
        "weighted avg       0.70      0.67      0.65      3000\n",
        "\n",
        "Results for COUNT_ADVERBS:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.68      0.82      0.74       750\n",
        "           2       0.65      0.29      0.40       750\n",
        "           3       0.56      0.91      0.70       750\n",
        "           4       0.90      0.66      0.76       750\n",
        "\n",
        "    accuracy                           0.67      3000\n",
        "   macro avg       0.70      0.67      0.65      3000\n",
        "weighted avg       0.70      0.67      0.65      3000\n",
        "\n",
        "Results for COUNT_PRONOUNS:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.68      0.82      0.74       750\n",
        "           2       0.66      0.29      0.40       750\n",
        "           3       0.57      0.91      0.70       750\n",
        "           4       0.89      0.67      0.76       750\n",
        "\n",
        "    accuracy                           0.67      3000\n",
        "   macro avg       0.70      0.67      0.65      3000\n",
        "weighted avg       0.70      0.67      0.65      3000\n",
        "\n",
        "Results for COUNT_MODALS:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.70      0.85      0.76       750\n",
        "           2       0.68      0.29      0.40       750\n",
        "           3       0.56      0.90      0.69       750\n",
        "           4       0.88      0.67      0.76       750\n",
        "\n",
        "    accuracy                           0.68      3000\n",
        "   macro avg       0.71      0.68      0.66      3000\n",
        "weighted avg       0.71      0.68      0.66      3000\n",
        "\n",
        "Results for COUNT_CONJUNCTIONS:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.69      0.81      0.75       750\n",
        "           2       0.66      0.30      0.42       750\n",
        "           3       0.57      0.91      0.70       750\n",
        "           4       0.89      0.67      0.77       750\n",
        "\n",
        "    accuracy                           0.67      3000\n",
        "   macro avg       0.70      0.67      0.66      3000\n",
        "weighted avg       0.70      0.67      0.66      3000\n",
        "\n",
        "Results for COUNT_PREPOSITIONS:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.69      0.82      0.75       750\n",
        "           2       0.66      0.29      0.40       750\n",
        "           3       0.57      0.92      0.70       750\n",
        "           4       0.89      0.66      0.76       750\n",
        "\n",
        "    accuracy                           0.67      3000\n",
        "   macro avg       0.70      0.67      0.65      3000\n",
        "weighted avg       0.70      0.67      0.65      3000\n",
        "\n",
        "Results for COUNT_INTERJECTIONS:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.69      0.83      0.75       750\n",
        "           2       0.67      0.29      0.41       750\n",
        "           3       0.57      0.92      0.70       750\n",
        "           4       0.88      0.66      0.75       750\n",
        "\n",
        "    accuracy                           0.67      3000\n",
        "   macro avg       0.70      0.67      0.65      3000\n",
        "weighted avg       0.70      0.67      0.65      3000\n",
        "\n",
        "Results for COUNT_DETERMINERS:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.69      0.82      0.75       750\n",
        "           2       0.66      0.30      0.41       750\n",
        "           3       0.57      0.91      0.70       750\n",
        "           4       0.87      0.66      0.75       750\n",
        "\n",
        "    accuracy                           0.67      3000\n",
        "   macro avg       0.70      0.67      0.65      3000\n",
        "weighted avg       0.70      0.67      0.65      3000\n",
        "\n",
        "Results for COUNT_NAMED_ENTITIES:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.68      0.78      0.73       750\n",
        "           2       0.64      0.30      0.41       750\n",
        "           3       0.56      0.90      0.69       750\n",
        "           4       0.89      0.70      0.78       750\n",
        "\n",
        "    accuracy                           0.67      3000\n",
        "   macro avg       0.69      0.67      0.65      3000\n",
        "weighted avg       0.69      0.67      0.65      3000\n",
        "\n",
        "Results for DIALOGUE_PERCENTAGE:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.65      0.77      0.71       750\n",
        "           2       0.62      0.30      0.41       750\n",
        "           3       0.55      0.86      0.67       750\n",
        "           4       0.88      0.69      0.77       750\n",
        "\n",
        "    accuracy                           0.66      3000\n",
        "   macro avg       0.68      0.66      0.64      3000\n",
        "weighted avg       0.68      0.66      0.64      3000\n",
        "\n",
        "Results for TEXT_SUBJECTIVITY:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.69      0.83      0.75       750\n",
        "           2       0.67      0.31      0.42       750\n",
        "           3       0.57      0.92      0.71       750\n",
        "           4       0.88      0.67      0.76       750\n",
        "\n",
        "    accuracy                           0.68      3000\n",
        "   macro avg       0.71      0.68      0.66      3000\n",
        "weighted avg       0.71      0.68      0.66      3000\n",
        "\n",
        "Results for TEXT_POLARITY:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           1       0.68      0.81      0.74       750\n",
        "           2       0.63      0.28      0.39       750\n",
        "           3       0.55      0.91      0.69       750\n",
        "           4       0.89      0.65      0.75       750\n",
        "\n",
        "    accuracy                           0.66      3000\n",
        "   macro avg       0.69      0.66      0.64      3000\n",
        "weighted avg       0.69      0.66      0.64      3000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for combined features (count_adjectives, count_modals, text_subjectivity):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.69      0.82      0.75       750\n",
            "           2       0.66      0.31      0.42       750\n",
            "           3       0.57      0.91      0.70       750\n",
            "           4       0.89      0.67      0.76       750\n",
            "\n",
            "    accuracy                           0.68      3000\n",
            "   macro avg       0.70      0.68      0.66      3000\n",
            "weighted avg       0.70      0.68      0.66      3000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Select only the desired feature extractors\n",
        "selected_features = {\n",
        "    'count_adjectives': FeatureExtractor['COUNT_ADJECTIVES'],\n",
        "    'count_modals': FeatureExtractor['COUNT_MODALS'],\n",
        "    'text_subjectivity': FeatureExtractor['TEXT_SUBJECTIVITY']\n",
        "}\n",
        "\n",
        "# Initialize containers for the selected feature data\n",
        "train_features_selected = []\n",
        "test_features_selected = []\n",
        "\n",
        "# Extract the selected features and store them in lists\n",
        "for feature_name, feature_func in selected_features.items():\n",
        "    train_feature = extract_feature_series(X_train, feature_func)\n",
        "    test_feature = extract_feature_series(X_test, feature_func)\n",
        "    train_features_selected.append(train_feature)\n",
        "    test_features_selected.append(test_feature)\n",
        "\n",
        "# Combine the selected features for training and testing sets\n",
        "# Use hstack to ensure compatibility with sparse matrices from TF-IDF\n",
        "X_train_combined_selected = hstack([X_train_tfidf] + train_features_selected)\n",
        "X_test_combined_selected = hstack([X_test_tfidf] + test_features_selected)\n",
        "\n",
        "# Train the RandomForest model with the combined selected features\n",
        "rf_model_selected = RandomForestClassifier(random_state=42)\n",
        "rf_model_selected.fit(X_train_combined_selected, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "predictions_selected = rf_model_selected.predict(X_test_combined_selected)\n",
        "report_selected = classification_report(y_test, predictions_selected, output_dict=True)\n",
        "\n",
        "# Print out the classification report for the combined features\n",
        "print(\"Results for combined features (count_adjectives, count_modals, text_subjectivity):\")\n",
        "print(classification_report(y_test, predictions_selected))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoqTqwitUUpc"
      },
      "source": [
        "# Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lGGmJfgUYW9",
        "outputId": "1bb2485b-503e-4c75-92ba-a372a7512c70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A little less than a decade ago, hockey fans were blessed with a slate of games every night, but on Thursday sources confirmed that for the ninth consecutive year NHL players have been locked out, with very slim hopes of an agreement in sight. It seems like just yesterday Martin St. Louis and his Lightning teammates were raising the Stanley Cup, high school hockey coach and onetime ESPN analyst Barry Melrose said. Obviously, Im still hoping the two sides can come together and reach an agreement, but Im starting to think nobody really misses hockey anymore. Nope. Nobody but old Barry. Id still love to catch an Atlanta Thrashers game. Observers have noted that when arena doors do reopen, the NHL will face the perhaps greater challenge of convincing fans to return to hockey instead of watching more popular sports like football, basketball, baseball, and SlamBall. \n"
          ]
        }
      ],
      "source": [
        "# Vectorizers\n",
        "# doc-to-vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW12FLGWUcI3"
      },
      "source": [
        "# Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dphIiZ1zUbxc"
      },
      "outputs": [],
      "source": [
        "model = ...\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPo0-5V-UlW_"
      },
      "source": [
        "# Model Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PAoqjhFUn9H"
      },
      "outputs": [],
      "source": [
        "f1score"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
